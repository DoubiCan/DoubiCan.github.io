---
title: 'Data-driven-Adaptive-mechanism'
date: 2025-09-05
private: 'true'
permalink: /posts/2025/09/blog-post-14/
tags:
  - Minkowski Metrics
  - statistics
---


此文随便推推Data-driven-Adaptive-mechanism




# 动机与总结

在把Medulla层直接替换为事件输入后,去看了下师兄的文章,觉得作为form一篇文章来说远远不够,数学远远不够,所以由是师兄的文章启发下有了下面的一点点推导

切入点是看到浩哥毕业论文的Haar部分的随机过程部分的理论应用就很羡慕,以及对`闵氏度量`这个度量的一些探索好奇心促使下推了一些小玩意。

但是最终由于`闵氏度量`相比于传统的二范数等等有一些缺陷，导致最终为了处理这些问题(非负性)使用了绝对值，以及建立理论过程中对时间维度的刻画导致理论退化为了马尔科夫链...

## GaussFilter Function and Random walking

先从一些经典的的理论引入

---
### GaussFilter Function

我们希望基于某个距离去定义一个分布的时候,无非就是希望找到一个函数\\(f:: \mathbb{R}^+ \rightarrow [0,1] \in \mathbb{R}\\)
我们希望离均值小的点概率大,希望离均值远的概率小,那么GaussFilter Function就是一个很适合的函数

$$
\frac{1}{\sqrt{2\pi}\sigma}exp(\frac{-||x-\mu||^2}{2\sigma^2})
$$

其中\\( \sigma \\)作用无非就是处理好我们要以多集中的概率给到均值附近的参数。并且整个函数简单来说就是用\\(e^{-x} \\)这个函数本身以及其自身的积分收敛性达到了这个目的。所以下面小节也自然会想把`闵氏度量`使用这个滤波函数来分配概率。

---
### Random walking

这个是说一个时间序列如题是由

$$
X(t+dt)=X(t)+\sigma dW(dt)
$$

通过Ito积分获得

$$
X(t)-X(0)=\int^{t}_{0}dx(s)=\int^{t}_{0}\sigma dW(t)=\sigma W(t)
$$

并且通过推导\\(X(t)\\)实际上是一个高斯过程i.e.任取有限个时间点的\\(X(t_i)\\)的联合分布为多元高斯分布

---

## Inspiration

### Metrics Change

为了引入`闵氏度量`，这里提供了两种办法(但是第一种并不是真的改度量而是形似)

---

### 带偏移的随机游走

一种是带偏移的随机游走。
$$
X(t+dt)=X(t)+\sqrt{\eta}dt+\sigma dW(dt)
$$
这种就是说既然`闵氏度量`希望在距离为0的地方概率最大，那我在仅仅关心那个位置也就是带上了\\(\sqrt{\eta}dt \\)偏移的随机游走也能保证对应位置概率最大。但是问题其实是反曲几何性质就丢失了。但是这样刻画的数学理论当然是漂亮的,因为这也是经典理论的东西。

最终通过Ito随机积分的结果是
$$
X(t) \sim N(X(0)+\sqrt{\vec{\eta}} t,\left[ \begin{array}{cc}\sigma_x^2t & 0\\0& \sigma_y^2\end{array}\right])
$$

以及不同时间点的协方差

$$
Cov(X(s),X(t))=\left[ \begin{array}{cc}Cov(X1(s),X1(t)) & Cov(X2(s),X1(t))\\Cov(X1(s),X2(t))& Cov(X2(s),X2(t))\end{array}\right] 
$$

$$
=\sigma^2 \left[ \begin{array}{cc}min(s,t) & 0\\0& min(s,t)\end{array}\right] 
$$

当我们获取到数据点Y后，有

$$
Y\sim N\left(\left[\begin{array}{c}t_{11}\sqrt{\eta_1}\\ t_{21}\sqrt{\eta_1} \\ t_{31}\sqrt{\eta_1}\\ ...\\ t_{11}\sqrt{\eta_2}\\ t_{21}\sqrt{\eta_2} \\ t_{31}\sqrt{\eta_2}\\ ... \end{array}\right],\left[\begin{array}{cc}\Sigma_1 & 0\\0& \Sigma_2\end{array} \right]\right)
$$

利用MLE求\\( \max ln \mathbb{P}(Y \| X,\eta,\sigma) \\)。即可求出\\(\eta\\)和\\(\sigma\\)的估计。

$$
\hat{\eta}_1=\frac{T^T\Sigma_1^{-1}Y_1+Y_1^T\Sigma_1^{-1}T}{2T^T\Sigma_1^{-1}T} 
$$

$$
\hat{\eta}_2=\frac{T^T\Sigma_2^{-1}Y_2+Y_2^T\Sigma_2^{-1}T}{2T^T\Sigma_2^{-1}T} 
$$

$$
\hat{\sigma}=...
$$

这里最终解释为\\(\eta\\)的大小是对object当前移动速度的一种体现，而\\(\sigma\\)的大小则反映了了这个物体的大小(更合适点说应该是边缘的大小)。

进而最终通过调整STMD中的OFF延迟参数\\(\tau\\)和侧抑制卷积核的\\(\sigma\\)参数来实时地适应变速运动和轻微物体大小变化。

甚至由于\\(X(t)\\)是解析的,以及不同时间点的多元高斯分布性,我们甚至能通过Bayes推导来压缩的可能性。i.e.对\\(\forall t^*\\)

$$
X(t^*) \sim N\left(\mu_{x|y},\Sigma_{x|y}\right)
$$

其中

$$
\mu_{x|y}=\mu_{x}+\Sigma_{xy}\Sigma_{yy}^{-1}(y-\mu_y)
$$

$$
\Sigma_{x|y}=\Sigma_{xx}+\Sigma_{xy}\Sigma_{yy}^{-1}\Sigma_{yx}
$$

其中\\(y\\)是数据。\\(x=X(t^*)\\)为随意点。通过bayes我们能拿到在给定数据下任一点的分布,至于作用嘛，可以用于预测未来事件点产生位置以及并在概率范围内人为增加事件数据来"增强数据"，冒充一个增强卷积核(全一)矩阵的作用等等

这部分的手稿如下
<div style="text-align: center;">
<div style="background-color: #d9d9d9ff; padding: 10px; display: inline-block;">
<img src='/images/Data_driven_Adaptive_mechanism/1.png'>
<p>Draft1</p>
</div>
</div>

---

### 闵氏度量高斯分布

另外一种就是选择把高斯滤波函数作为概率分配的载体，将度量函数直接更替掉，形成了一个新的分布。

$$
\mathbb{P}((x,y)|dt,\eta,\sigma,x_i,y_i,t_i)=\frac{1}{C}exp(\frac{-|(x-x_i)^2+(y-y_i)^2-\eta*dt^2|}{2\sigma^2})
$$

其中\\(C\\)用于归一化概率的常数。这里使用绝对值来保证非负性，但这样做会有个缺点是对于闵氏距离为正数(如0.5)和负数(如-0.5)的概率分布会是同待遇,但实际上我们宁愿希望它为正数(因果性)也不希望为负数(无因果)，因此更好的调整可能是双边不同\\( \sigma^+-\\) 和\\(\sigma^-\\)(待定)

$$
C=2\pi\sigma^2\left[w^{-1}exp(\frac{R^2}{2\sigma^2})-w^{-1}+w\cdot exp(\frac{-R^2}{2\sigma^2}) \right]
$$

$$
w=exp(\frac{\eta dt^2}{2\sigma^2})
$$

然后本来我是希望\\( X(t+dt)=X(t)+dX\\)类似地随机积分然后得到显式的\\(X(t)\\)解析形式,但由于dX服从上面那个分布i.e.与当前状态X(t)有关，然后好像没有什么解析解,只能离散开来:

$$
X(t)=\sum_{t_i=0}^N dX(t_i)+X(0) 
$$

到这里我们仍然希望写出似然函数来求对\\(\eta\\)和\\(\sigma\\)的估计。我们注意到\\( dX(t_i)\\)依赖于\\(X(t_i)\\)来计算闵氏距离而不依赖之前的状态\\(X(t_{i-1})...\\),因此实际上就是马尔可夫链！所以可以写出似然函数:

$$
\begin{aligned}
\mathbb{P}(X_0,X_1,X_2,X_3...|\theta)& =\mathbb{P}(X_0)\mathbb{P}(X_1|X_2)\mathbb{P}(X_1|X_2,X_3)...\\
&=\mathbb{P}(X_0)\prod_{i=1}^N \mathbb{P}(X_i|X_{i-1}) \\
&=\mathbb{P}(X_0) \prod_{i=1}^N \mathbb{P}(dX_i|X_{i-1}) \\
&=\frac{1}{\prod_{i=1}^NC_i}exp\left\{ \frac{-\sum_{i=0}^N |dx_i^2+dy_i^2-\eta dt_i^2|}{2\sigma^2} \right\}
\end{aligned}
$$

通过令这个函数的对数的次梯度等于0得到对二者的估计。

部分手稿
<div style="text-align: center;">
<div style="background-color: #d9d9d9ff; padding: 10px; display: inline-block;">
<img src='/images/Data_driven_Adaptive_mechanism/2.png'>
<p>Draft2</p>
</div>
</div>

---

### 圆环分布

这里再补充一部分内容,是上面第一部分`带偏移的随机游走`的深度优化版本。我们先看两组事件数据,二者都是使用\\(\alpha=5,sigma=0\\)生成的数据,即使用生成的参数分别是\\(v_x=3,v_y=4\\)与\\(R=1,f=5\\)这些参数的目的都是为了:
<div style="text-align: center;">
<img src='/images/Data_driven_Adaptive_mechanism/Liner_Spiral_Motion_Data.png'>
<p>人工生成事件数据</p>
</div>

对于这种数据,如果仍然使用上面带偏移的随机游走,虽然说有Ito积分可以保证能积分出\\(X(t)\\)的解析解,这是高斯分布的优越性质,但是之前这里的偏移向量\\(\eta\\\)是一个常量,即它只能对上面图中左边这种数据有比较好的估计(对\\(\eta\\) ).而对于右边这种数据实际上由于\\(\eta\\)向量不是常量来说的数据来说就寄了.因此我们得改一改。

我们依旧回想一下我们引入闵氏距离的目的是什么:是希望下一个数据点落在前面一个数据点为中心的一个圆环的环上,圆的半径由\\(dt_i\\)决定。如图所示

<div style="text-align: center;">
<img src='/images/Data_driven_Adaptive_mechanism/3.png'>
<p>圆环高斯分布</p>
</div>

既然这样我们自然而然就引入下面这个概率函数作为我们的\\(p((x_{i},y_{i})\|(x_{i-1},y_{i-1}))\\)

$$
f(x,y)=\frac{1}{Z}exp(\frac{-(r-R)^2}{2\sigma^2})
$$

经过计算得到归一化\\(Z\\)

$$
Z=2\pi \left\{ \sigma^2 exp(\frac{-R^2}{2\sigma^2})+R\sigma\sqrt{2\pi}\Phi(\frac{R}{\sigma}) \right\}
$$

似然函数就可以写出来

$$
\begin{aligned}
\max \ln \mathbb{P}&=-\sum_{i=1}^{N} \ln Z_i - \frac{\sum_{i=1}^{N} (\sqrt{dx_i^2 + dy_i^2} - \alpha dt_i)^2}{2\sigma^2} \\
&= \cdots \\
&=- N \cdot \ln 2\pi - \sum_{i=1}^{N} \ln \left\{ \sigma^{2} \exp \left( -\frac{\alpha^{2} dt_{i}^{2}}{2\sigma^{2}} \right) + R \sigma \sqrt{2\pi} \Phi \left( \frac{\alpha dt_{i}}{\sigma} \right) \right\} - \frac{1}{2\sigma^{2}} \sum_{i=1}^{N} \left( \sqrt{dx_{i}^{2} + dy_{i}^{2}} - \alpha dt_{i} \right)^{2}
\end{aligned}
$$

通过求解最大化似然函数找到对\\(\alpha\\)和\\(\sigma\\)的估计。这种就可以克服原本常量向量偏移对第二种数据的不适用。但是作为代价，就是没有解析解，只能写出离散的似然函数，即上面需要被最大化的函数。

这里可以看看这个函数的三维曲面图。由于上面生成数据是完全精确的不带噪声的即\\(\sigma=0\\)所以这个曲面关于\\(\sigma\\)的最大值的位置是0.

<div style="text-align: center;">
<img src='/images/Data_driven_Adaptive_mechanism/Log_Likelihood_surface.png'>
<p>似然函数三维曲面图</p>
</div>

然后是通过BFGS求解的结果

<div style="text-align: center;">
<img src='/images/Data_driven_Adaptive_mechanism/4.png'>
<p>BFGS求解</p>
</div>

可以看到我这里求出的\\(\alpha\\)和\\(\sigma\\)就是所需要的，其含义就是上面几个小节说的速度和大小。

小总结一下这里的补充是对上面带偏移随机游走的补充,仍然是``与闵氏度量的目的一致,但并没有使用闵氏度量""。其目的一致体现在希望在上一个数据点为圆心的圆的圆边上是下一个数据点所出现的位置！

这部分的手稿如下
<div style="text-align: center;">
<div style="background-color: #d9d9d9ff; padding: 10px; display: inline-block;">
<img src='/images/Data_driven_Adaptive_mechanism/5.png'>
<p>Draft</p>
</div>
</div>

---

## 总结

  1. 推导总结
      * 用概率的方式的方式刻画闵氏度量的参数\\(\eta\\)并通过MLE来利用数据估计出这个值,这其实我感觉啊在不退化为马尔科夫链之前我还挺有信心的感觉会很漂亮，但是退化为马尔科夫链之后我有感觉这跟直接用一条空间中的线去拟合三维空间中的数据点有什么区别(第三维是t)。无非就是把拟合的度量换掉而已
  2. 是否有用
      * 本来觉得推完应该勉强能用的数学，结果感觉还是不够用，还是太浅了，上不了桌，起码现在的不行.
  2. Thought
      * 要不要先写着算法，然后整个变速运动的目标试着先?有必要吗?

    


<!-- $$ 
0 = \frac{\partial}{\partial \alpha} = -\sum_{i=1}^{N} \frac{\phi(\frac{\alpha d t_{i}}{\sigma}) \cdot \frac{d t_{i}}{ \sigma }}{\Phi(\frac{\alpha d t_{i}}{\sigma})} + \frac{1}{ \sigma^2 } \sum_{i=1}^{N} d t_{i}^{2}(\sqrt{\frac{d x_{i}^{2}+d y_{i}^{2}}{d t_{i}^{2}}}-\alpha) 
 $$

$$ 
0 = \frac{\partial}{\partial \sigma} =-\frac{N}{\sigma} +\sum_{i=1}^{N} \frac{\phi(\frac{\alpha d t_{i}}{\sigma}) \cdot \frac{\alpha d t_{i}}{ \sigma^2 }}{\Phi(\frac{\alpha d t_{i}}{\sigma})} + \frac{1}{ \sigma^3 } \sum_{i=1}^{N} d t_{i}^{2}(\sqrt{\frac{d x_{i}^{2}+d y_{i}^{2}}{d t_{i}^{2}}}-\alpha)^2
 $$

$$ 
\mathcal{ln}\mathbb{P}(\alpha, \sigma)  = -N \ln(2\pi)- \sum_{i=1}^{N} \ln \left\{  \sigma^2exp(\frac{-(\alpha dt_i)^2}{2\sigma^2}) + R\sigma \sqrt{2\pi} \Phi( \frac{\alpha dt_i}{\sigma} ) \right\} - \frac{1}{2 \sigma^{2}} \sum_{i=1}^{N} dt_i^{2} \left( \sqrt{ \frac{dx_i^2 + dy_i^2}{dt_i^2} } - \alpha \right)^2
$$ -->