---
title: 'Motion Deblur Review'
date: 2025-07-26
private: 'true'
permalink: /posts/2025/07/blog-post-10/
tags:
  - Review
  - EVENT
  - Motion Deblur
---


此文对Motion Deblur recent work 进行 Review




# 总结

先说结论,`Event Deblur`的工作从2017到现在都已经做了很多了，包括从数学理论出发的[[4]](#ref4)[[5]](#ref5)[[6]](#ref6),又或是直接从深度网络出发[[3]](#ref3)。再或者将二者结合形成可解释性浅层网络型的[[6]](#ref6)。

再进行一波承上:之前我们讨论过引入光流方程使得event和frame之间得以建立一个优化问题框架下的桥梁。但是在下面进行Review过程中我们会逐渐明白这个桥梁是非常地相形见绌。以至于我都不知道要不要再继续建造下去。

## Math And Event Deblur

实际上Event 与 Frame之间的严格数学关系已经被摸得很透彻了。这一节当中我们先Review几篇涉及到了这些数学公式关系的"开山鼻祖"的文章

---
### [《Bringing a blurry frame alive at high frame-rate with an event camera》EDI_CVPR_2019_中科院一区]() [[4]](#ref4)

EDI这篇文章实际上直接从`事件生成原理`与`运动模糊产生原理`的数学原理直接给出了运动模糊图像和事件和清晰图像的关系，并且这个关系不需要任何的假设(除了仅仅要求事件相机能达到的的精度是完全准确的理论精度以外)。
<div style="text-align: center;">
  <img src='/images/Event_Deblur_Review/EDI_math.png'>
  <p>EDI_Math</p>
</div>
其中\[B\]为观测到的运动模糊的图像;\[L_f\]为DAVIS的在某帧的图像;\[e(s)\]即为 `事件极性函数` 在时间点s除的值。

若是假定产生事件的阈值`c`是固定的，那这里使用这个公式就可以直接通过`B`和`e(S)`计算出真实图像\[L_f\]。
但是这里为了模型更为健壮，也为了考虑到实际上的`c`由于光源热源误差导致会变化等原因进而假设为随机数，即作为优化变量去达到优化目标:

<div style="text-align: center;">
  <img src='/images/Event_Deblur_Review/EDI_math2.png'>
  <p>EDI_Math2</p>
</div>

这里就是增加了一个优化变量以调控对去模糊效果的`DIY`罢,其中\[M(t)\]是为了去噪;TV是为了锐化;EDGE是为了事件与图像的边缘的对齐;

总而言之这个是最为严格的数学推导，作用是给出传感器得到的运动模糊图与真实帧与事件之间的关系，而不需要再额外需要假设满足光流方程！

---
### [《Event Enhanced High-Quality Image Recovery》SPARSE-Frame_ECCV_2020_中科院二区]() [[6]](#ref6)

这篇文章更是重量级！直接给我当头一棒！这篇是把稀疏的那个框架用上了。原文是说稀疏框架将带来无与伦比的抗噪能力。

同样地根据上面EDI中那个模糊帧\[B\] 与 GT \[L_f\]的关系有,
<div style="text-align: center;">
  <img src='/images/Event_Deblur_Review/Sparse_Framework_math1.png'>
  <p>Sparse_Framework_math1</p>
</div>

再加入可能的传感器噪声,以及更为仔细地将传感器的作为离散采样器的下采样局限性也建模后即可得到
<div style="text-align: center;">
  <img src='/images/Event_Deblur_Review/Sparse_Framework_math2.png'>
  <p>Sparse_Framework_math2</p>
</div>

其中\[E\]是上面简写的那个算子;\[P\]是代表传感器离散采样的下采样算子;

在引入稀疏框架的经典假设`图像Y(or X)是可被各自的字典稀疏表示的`,以及注意到\[Y\]与\[X\]间是线性关系，可以知道他们是在各自字典的表示下是共用系数的([笔记的CH15.4部分](https://doubican.github.io/posts/2025/05/blog-post-8/))。在这个经典稀疏框架的假设下就可以引出下面优化问题
<div style="text-align: center;">
  <img src='/images/Event_Deblur_Review/Sparse_Framework_math3.png'>
  <p>Sparse_Framework_math3</p>
</div>
这里实际上就是在求模糊图在模糊字典(假设给定,反正在压缩感知的教材上这种是通过对不同的低分辨或者高分辨上使用KSVD得到的)上的表示系数\[ \alpha \]，
再利用共享系数的性质在清晰字典\[D_X\]上重新表示即可。这里一范数实际上是对零范数的替代

这个问题的求解详见[笔记中补充材料的SSF算法](https://doubican.github.io/posts/2025/05/blog-post-8/)，最终便得到迭代式：

<div style="text-align: center;">
  <img src='/images/Event_Deblur_Review/Sparse_Framework_math4.png'>
  <p>Sparse_Framework_math4</p>
</div>

稀疏框架(字典表示框架)下的数学就到这了。但这篇文章还有一点题外话东西kick my mind 的就是他还希望将这个东西与CNN结合(其实是被其他一篇文章启示的)，就是说在上面那个优化问题下给出的迭代式，原话是说"非负软阈值算子的作用实际上和机器学习的激活函数作用很像"所以可以将软阈值算子以ReLU层替代，卷积层实际上也是线性操作所以作为矩阵乘积的替代，加减就有普通的加减替代,积分就用加权求和卷积代替，然后构建了下图所谓的可解释性的去模糊深度深度网络"，下面这个网络每个结构其实是一一对应于上面那个迭代式的！(我应该去看看它代码是怎么实现的！)

<div style="text-align: center;">
  <img src='/images/Event_Deblur_Review/Sparse_Framework_math5.png'>
  <p>Sparse_Framework_math5</p>
</div>

意思就是说我们在研究某个问题时构造出了优化问题，并且通过某些算法得到了一些迭代式后，就可以类似这样地构造一个"可解释的""训练型"的网络!就是说成功地将优化问题的迭代求解变成了一种训练网络的过程，你是知道的，对于一个训练好的网络，它最大的好处就是快！能实时！！！这是优化算法不能做到的。

---
### [《Real-Time Intensity-Image Reconstruction for Event Cameras Using Manifold Regularisation》Event_Manifold_IJCV_2018_中科院二区]()[[5]](#ref5)

这篇文章虽然不属于Deblur Event Review而是属于Event Reconstruction范畴 但是它涉及到了一些关于`事件流形`的数学理论,刚好是老师上次提到的,所以也看了两眼。文章中内容启示颇多，这里只是Review就简单罗列一下。
<div style="text-align: center;">
  <img src='/images/Event_Deblur_Review/Event_Manifold_math1.png'>
  <p>Event_Manifold_math1</p>
</div>
这个是它对事件流形的定义，其中\[t(x,y)\]是指\[(x,y)\]位置的最新的事件的时间\[t\]。在这个定义后就可以定义除这个流形上任何一个泛函(其实就是图像函数)的切空间梯度。
<div style="text-align: center;">
  <img src='/images/Event_Deblur_Review/Event_Manifold_math2.png'>
  <p>Event_Manifold_math2</p>
</div>

随后就可以定义在这个流形上的TV范数，并且考虑到\[R^3\]到流形上的变量代换从流形上的优化问题拉回了原空间的优化问题。
<div style="text-align: center;">
  <img src='/images/Event_Deblur_Review/Event_Manifold_math3.png'>
  <p>Event_Manifold_math3</p>
</div>
后面是对这个优化问题的离散化以及对偶求解方法(解决正常梯度算法在TV范数取一范数时的不可导问题)。见[笔记]()

## NetWork And Event Deblur
除了数学理论外，更多的对于Event Deblur的推荐还是深度网络的架构，涉及数学基本上没有基本上都是网络框架的设计，所以就不细说了。
---
### [《Learning to deblur and generate high frame rate video with an event camera2020事件去模糊深度网络》CVPR_2019_中科院一区]()[[2]](#ref2)
深度网络架构
---
### [《Frequency-aware_Event-based_Video_Deblurring_for_Real-World_Motion_Blur》CVPR_2024_中科院一区]()[[1]](#ref1)
深度网络架构,比较新奇的地方是它在某个模块中将张量转换到频域去进行卷积，再转换回时域，这样可以避免卷积核大小的设计，也提供了多可能的网络层的设计，(可能他发现在频域进行网络设计有奇效吧)(那不是要训练整个张量大小的矩阵作为频域的那个卷积函数对应的那个矩阵吗？参数不会过多吗？)
---
### [《EFI-Net_Video_Frame_Interpolation_From_Fusion_of_Events_and_Frames》CVPR_2021_中科院一区]()[[3]](#ref3)
深度网络架构，给到一点提示是这里面提到了除了RGB外的另外一个颜色数据格式CIELAB COLOR SPACE。这是一个由[强度，红绿轴，蓝黄轴]这样的，因此灰度帧与彩色帧可以直接处理这个颜色空间的第一个通道即可，代替掉以前只处理RGB其中一个通道的问题。

## 总结

  1. 关于Event Deblur
      * 关于光流:实际上整个解决这个问题上，所有人几乎都在尽可能地避免计算光流，并且有EDI那个更为严格的数学推导，所以说假设服从光流方程可能得再斟酌斟酌?
      * 上面将事件应用于稀疏框架实际上简直就是我梦中的自己！前人砍树，后人暴晒，前人摘果，后人饿死！
  2. 其他
      * 事件流形的建立实际上也为我们提供了一种能直接处理事件数据的方法，而不是仍旧将其进行帧结构化这种有点有点像阶梯函数的映射方法。至于会不会有更好的效果我只能说待定了，但起码数学上很漂亮。
      * 以及在上面关于稀疏框架下的事件模型构建的将优化问题转化为可训练的CNN网络也是一个很大的expand研究方法，可能优化问题解决不好但是能通过数据量的叠加使得网络能处理好...嘻嘻


<!-- %------------------------------------------------------------------------ -->


## 参考文献
<a id="ref1"></a>
 <a href="https://DoubiCan.github.io/files/Kim_Frequency-aware_Event-based_Video_Deblurring_for_Real-World_Motion_Blur_CVPR_2024_paper_一篇比较新的用深度网络解决去模糊问题.pdf">**[1]** Kim T, Cho H, Yoon K J. Frequency-aware event-based video deblurring for real-world motion blur[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 24966-24976.</a>
<a id="ref2"></a>  
 <a href="https://DoubiCan.github.io/files/Learning%to%deblur%and%generate%high%frame%rate%video%with%an%event%camera2020事件去模糊深度网络.pdf">**[2]** Haoyu C, Minggui T, Boxin S, et al. Learning to deblur and generate high frame rate video with an event camera[J]. arXiv preprint arXiv:2003.00847, 2020.</a>
 <a id="ref3"></a>  
 <a href="https://DoubiCan.github.io/files/Paikin_EFI-Net_Video_Frame_Interpolation_From_Fusion_of_Events_and_Frames_CVPRW_2021_paper2021事件去模糊深度网络.pdf">**[3]** Paikin G, Ater Y, Shaul R, et al. Efi-net: Video frame interpolation from fusion of events and frames[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021: 1291-1301.</a>
 <a id="ref4"></a>  
 <a href="https://DoubiCan.github.io/files/Pan_Bringing_a_Blurry_Frame_Alive_at_High_Frame-Rate_With_an_CVPR_2019_paper_EDI.pdf">**[4]** Pan L, Scheerlinck C, Yu X, et al. Bringing a blurry frame alive at high frame-rate with an event camera[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019: 6820-6829.</a>
  <a id="ref5"></a>  
 <a href="https://DoubiCan.github.io/files/Real-Time%Intensity-Image%Reconstruction%for%Event%Cameras%Using%Manifold%Regularisation2018事件时间面流形_图像强度重建IJCV中科院2区.pdf">**[5]** Munda G, Reinbacher C, Pock T. Real-time intensity-image reconstruction for event cameras using manifold regularisation[J]. International Journal of Computer Vision, 2018, 126(12): 1381-1393.</a>
  <a id="ref6"></a>  
 <a href="https://DoubiCan.github.io/files/Event%Enhanced%High-Quality%Image%Recovery2020事件去模糊稀疏框架ECCV中科院2区.pdf">**[6]** Wang B, He J, Yu L, et al. Event enhanced high-quality image recovery[C]//European Conference on Computer Vision. Cham: Springer International Publishing, 2020: 155-171.</a>

